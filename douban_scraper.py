import requests
from bs4 import BeautifulSoup
from typing import Optional, Dict
import time
import urllib.parse
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
import hashlib
from functools import lru_cache

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


class DoubanScraper:
    """æœ€å¼ºå¥ç‰ˆè±†ç“£å›¾ä¹¦æœç´¢ï¼ˆå¸¦ç¼“å­˜ä¼˜åŒ–ï¼‰"""

    # ç±»çº§åˆ«çš„ç¼“å­˜å­—å…¸ï¼Œæ‰€æœ‰å®ä¾‹å…±äº«
    _cache = {}
    _cache_max_size = 1000  # æœ€å¤šç¼“å­˜1000ä¸ªç»“æœ
    _cache_ttl = 3600  # ç¼“å­˜1å°æ—¶

    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Connection': 'keep-alive',
            'Referer': 'https://www.douban.com/',
            'Upgrade-Insecure-Requests': '1',
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)

    @staticmethod
    def _get_cache_key(title: str, author: str = None, publisher: str = None) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_parts = [title.strip().lower()]
        if author:
            key_parts.append(author.strip().lower())
        if publisher:
            key_parts.append(publisher.strip().lower())
        key_str = ':'.join(key_parts)
        return hashlib.md5(key_str.encode('utf-8')).hexdigest()

    @classmethod
    def _get_from_cache(cls, cache_key: str) -> Optional[Dict]:
        """ä»ç¼“å­˜è·å–ç»“æœ"""
        if cache_key in cls._cache:
            cached_data, timestamp = cls._cache[cache_key]
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            if time.time() - timestamp < cls._cache_ttl:
                logger.info(f"  ğŸ’¾ ç¼“å­˜å‘½ä¸­: {cache_key[:8]}...")
                return cached_data
            else:
                # è¿‡æœŸåˆ™åˆ é™¤
                del cls._cache[cache_key]
        return None

    @classmethod
    def _save_to_cache(cls, cache_key: str, data: Dict):
        """ä¿å­˜åˆ°ç¼“å­˜"""
        # å¦‚æœç¼“å­˜å·²æ»¡ï¼Œåˆ é™¤æœ€æ—§çš„10%
        if len(cls._cache) >= cls._cache_max_size:
            sorted_keys = sorted(cls._cache.keys(), key=lambda k: cls._cache[k][1])
            for key in sorted_keys[:cls._cache_max_size // 10]:
                del cls._cache[key]

        cls._cache[cache_key] = (data, time.time())
        logger.info(f"  ğŸ’¾ å·²ç¼“å­˜ç»“æœ: {cache_key[:8]}... (ç¼“å­˜æ•°: {len(cls._cache)})")

    def search_book(self, title: str, author: str = None, publisher: str = None, include_comments: bool = False) -> Optional[Dict]:
        """
        æœç´¢ä¹¦ç±ä¿¡æ¯ï¼Œä½¿ç”¨å¤šç§ç­–ç•¥ï¼ˆå¹¶è¡Œæ‰§è¡Œï¼‰+ ç¼“å­˜

        Args:
            title: ä¹¦å
            author: ä½œè€…ï¼ˆå¯é€‰ï¼‰
            publisher: å‡ºç‰ˆç¤¾ï¼ˆå¯é€‰ï¼‰
            include_comments: æ˜¯å¦åŒ…å«çŸ­è¯„ï¼ˆé»˜è®¤Falseï¼Œæå‡æ€§èƒ½ï¼‰
        """
        search_start = time.time()

        # ç”Ÿæˆç¼“å­˜é”®ï¼ˆä¸åŒ…å«çŸ­è¯„æ ‡å¿—ï¼ŒçŸ­è¯„å•ç‹¬è·å–ï¼‰
        cache_key = self._get_cache_key(title, author, publisher)

        # æ£€æŸ¥ç¼“å­˜
        cached_result = self._get_from_cache(cache_key)
        if cached_result:
            # å¦‚æœéœ€è¦çŸ­è¯„ä¸”ç¼“å­˜ä¸­æ²¡æœ‰ï¼Œåˆ™è·å–çŸ­è¯„
            if include_comments and not cached_result.get('short_comments'):
                if cached_result.get('url'):
                    comment_start = time.time()
                    logger.info("  ğŸ’¬ è·å–çŸ­è¯„...")
                    cached_result['short_comments'] = self._get_short_comments(cached_result['url'])
                    comment_time = (time.time() - comment_start) * 1000
                    logger.info(f"  âœ… çŸ­è¯„è·å–å®Œæˆ: {comment_time:.2f}ms")

            total_time = (time.time() - search_start) * 1000
            logger.info(f"  â±ï¸  ç¼“å­˜æŸ¥è¯¢æ€»è€—æ—¶: {total_time:.2f}ms")
            return cached_result

        logger.info(f"  ğŸ” å¼€å§‹å¹¶è¡Œæœç´¢: {title}")

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œå¤šä¸ªæœç´¢ç­–ç•¥
        result = None

        with ThreadPoolExecutor(max_workers=2) as executor:
            # æäº¤ä¸¤ä¸ªæœç´¢ä»»åŠ¡
            logger.info("  âš¡ å¹¶è¡Œæäº¤2ä¸ªæœç´¢ç­–ç•¥...")
            future_web = executor.submit(self._search_douban_web, title, author)
            future_book = executor.submit(self._search_douban_book, title, author)

            # ç­‰å¾…ä»»ä¸€ä»»åŠ¡å®Œæˆå¹¶è¿”å›æœ‰æ•ˆç»“æœï¼ˆä½¿ç”¨æ›´çŸ­çš„è¶…æ—¶ï¼‰
            for future in as_completed([future_web, future_book], timeout=8):  # å‡å°‘åˆ°8ç§’
                try:
                    temp_result = future.result()
                    if temp_result:
                        result = temp_result
                        elapsed = (time.time() - search_start) * 1000
                        logger.info(f"  âœ… å¹¶è¡Œæœç´¢æˆåŠŸ: {result.get('source')} ({elapsed:.2f}ms)")
                        # å–æ¶ˆå…¶ä»–æœªå®Œæˆçš„ä»»åŠ¡
                        break
                except Exception as e:
                    logger.error(f"  âŒ æœç´¢ç­–ç•¥å¼‚å¸¸: {e}")
                    continue

        # å¦‚æœå¹¶è¡Œç­–ç•¥éƒ½å¤±è´¥ï¼Œä½¿ç”¨å…œåº•æ–¹æ¡ˆ
        if not result:
            logger.warning("  âš ï¸  æ‰€æœ‰æœç´¢ç­–ç•¥å¤±è´¥ï¼Œä½¿ç”¨å…œåº•æ–¹æ¡ˆ")
            result = self._create_fallback_result(title, author, publisher)

        # ä¿å­˜åˆ°ç¼“å­˜
        if result:
            self._save_to_cache(cache_key, result)

        # åªåœ¨æ˜ç¡®è¦æ±‚æ—¶æ‰è·å–çŸ­è¯„
        if result and include_comments and result.get('url'):
            comment_start = time.time()
            logger.info("  ğŸ’¬ è·å–çŸ­è¯„...")
            result['short_comments'] = self._get_short_comments(result['url'])
            comment_time = (time.time() - comment_start) * 1000
            logger.info(f"  âœ… çŸ­è¯„è·å–å®Œæˆ: {comment_time:.2f}ms")

        total_time = (time.time() - search_start) * 1000
        logger.info(f"  â±ï¸  å¹¶è¡Œæœç´¢æ€»è€—æ—¶: {total_time:.2f}ms")

        return result

    def _search_douban_web(self, title: str, author: str = None) -> Optional[Dict]:
        """é€šè¿‡è±†ç“£æœç´¢é¡µé¢æŸ¥æ‰¾ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        try:
            query = title.strip()
            search_url = f"https://www.douban.com/search?cat=1001&q={urllib.parse.quote(query)}"

            print(f"å°è¯•è®¿é—®: {search_url}")

            # ä¼˜åŒ–çš„é‡è¯•æœºåˆ¶ï¼šæœ€å¤š1æ¬¡é‡è¯•ï¼Œå¿«é€Ÿå¤±è´¥
            max_retries = 1  # å‡å°‘åˆ°1æ¬¡é‡è¯•
            response = None
            for attempt in range(max_retries + 1):
                try:
                    # ç¬¬ä¸€æ¬¡å°è¯•ç”¨æ›´çŸ­çš„è¶…æ—¶
                    timeout = 5 if attempt == 0 else 7  # 5ç§’æˆ–7ç§’
                    response = self.session.get(search_url, timeout=timeout)
                    print(f"å“åº”çŠ¶æ€: {response.status_code}")
                    if response.status_code == 200:
                        break
                except Exception as e:
                    print(f"ç¬¬{attempt + 1}æ¬¡å°è¯•å¤±è´¥: {e}")
                    if attempt == max_retries:
                        raise
                    time.sleep(0.5)  # é‡è¯•ç­‰å¾…å‡å°‘åˆ°0.5ç§’

            if not response or response.status_code != 200:
                print(f"æœç´¢è¯·æ±‚å¤±è´¥: {response.status_code if response else 'No response'}")
                return None

            soup = BeautifulSoup(response.text, 'html.parser')

            # å¤šç­–ç•¥æŸ¥æ‰¾ç»“æœ
            book_info = self._extract_with_multiple_strategies(soup, title, author)
            if book_info:
                return book_info

        except Exception as e:
            print(f"è±†ç“£æœç´¢å¼‚å¸¸: {e}")

        return None

    def _extract_with_multiple_strategies(self, soup, title: str, author: str = None) -> Optional[Dict]:
        """ä½¿ç”¨å¤šç§ç­–ç•¥æå–ä¹¦ç±ä¿¡æ¯"""

        # ç­–ç•¥1: æŸ¥æ‰¾æ ‡å‡†div.resultç»“æ„
        results = soup.find_all('div', class_='result')
        if results:
            print(f"ç­–ç•¥1: æ‰¾åˆ° {len(results)} ä¸ªdiv.result")
            for result in results[:3]:
                book_info = self._extract_from_standard_result(result, title)
                if book_info:
                    return book_info

        # ç­–ç•¥2: æŸ¥æ‰¾æ‰€æœ‰åŒ…å«book.douban.comçš„é“¾æ¥
        print("ç­–ç•¥2: æŸ¥æ‰¾ä¹¦ç±é“¾æ¥")
        all_links = soup.find_all('a', href=True)
        book_links = []

        for link in all_links:
            href = link.get('href', '')
            text = link.get_text().strip()

            # æ£€æŸ¥æ˜¯å¦æ˜¯ä¹¦ç±é“¾æ¥
            if 'book.douban.com/subject' in href or 'link2' in href:
                # æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«ç›®æ ‡ä¹¦å
                if self._text_contains_title(text, title):
                    book_links.append((link, text, href))

        if book_links:
            print(f"æ‰¾åˆ° {len(book_links)} ä¸ªæ½œåœ¨ä¹¦ç±é“¾æ¥")
            for link, text, href in book_links[:3]:
                book_info = self._extract_from_link_context(link, title, author)
                if book_info:
                    return book_info

        # ç­–ç•¥3: æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾
        print("ç­–ç•¥3: æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾")
        page_text = soup.get_text()
        if title in page_text:
            # ä½¿ç”¨æ­£åˆ™æŸ¥æ‰¾å¯èƒ½çš„é“¾æ¥
            pattern = rf'(https://book\.douban\.com/subject/\d+/)'
            matches = re.findall(pattern, response.text if hasattr(self, 'response') else str(soup))
            if matches:
                return {
                    'title': title,
                    'author': author or '',
                    'publisher': '',
                    'rating': None,
                    'url': matches[0],
                    'source': 'regex_match'
                }

        return None

    def _extract_from_standard_result(self, result_elem, target_title: str) -> Optional[Dict]:
        """ä»æ ‡å‡†div.resultç»“æ„æå–ä¿¡æ¯"""
        try:
            content_div = result_elem.find('div', class_='content')
            if not content_div:
                return None

            title_div = content_div.find('div', class_='title')
            if not title_div:
                return None

            h3_elem = title_div.find('h3')
            if not h3_elem:
                return None

            title_link = h3_elem.find('a', href=True)
            if not title_link:
                return None

            found_title = title_link.get_text(strip=True)
            clean_title = found_title.replace('[ä¹¦ç±]', '').strip()
            clean_title = re.sub(r'\s+', ' ', clean_title)

            if self._is_title_match(target_title, clean_title):
                link_url = title_link.get('href')

                # å¤„ç†è·³è½¬é“¾æ¥
                if link_url and 'link2' in link_url:
                    match = re.search(r'url=([^&]+)', link_url)
                    if match:
                        real_url = urllib.parse.unquote(match.group(1))
                        link_url = real_url

                # æå–è¯„åˆ†å’Œä½œè€…ä¿¡æ¯
                rating = None
                author = ''
                publisher = ''

                rating_div = title_div.find('div', class_='rating-info')
                if rating_div:
                    rating_elem = rating_div.find('span', class_='rating_nums')
                    if rating_elem:
                        try:
                            rating = float(rating_elem.get_text(strip=True))
                        except:
                            pass

                    subject_cast = rating_div.find('span', class_='subject-cast')
                    if subject_cast:
                        cast_text = subject_cast.get_text(strip=True)
                        parts = [p.strip() for p in cast_text.split('/')]
                        if len(parts) >= 1:
                            author = parts[0]
                        if len(parts) >= 2:
                            publisher = parts[1]

                return {
                    'title': clean_title,
                    'author': author,
                    'publisher': publisher,
                    'rating': rating,
                    'url': link_url or '',
                    'source': 'douban'
                }

        except Exception as e:
            print(f"æ ‡å‡†ç»“æ„æå–å¤±è´¥: {e}")

        return None

    def _extract_from_link_context(self, link, target_title: str, author: str = None) -> Optional[Dict]:
        """ä»é“¾æ¥ä¸Šä¸‹æ–‡æå–ä¿¡æ¯"""
        try:
            text = link.get_text(strip=True)
            href = link.get('href')

            # å¤„ç†è·³è½¬é“¾æ¥
            if 'link2' in href:
                match = re.search(r'url=([^&]+)', href)
                if match:
                    real_url = urllib.parse.unquote(match.group(1))
                    href = real_url

            # æ¸…ç†æ ‡é¢˜
            clean_title = text.replace('[ä¹¦ç±]', '').strip()

            if self._is_title_match(target_title, clean_title):
                # å°è¯•ä»çˆ¶å…ƒç´ è·å–æ›´å¤šä¿¡æ¯
                rating = None
                author_info = ''
                publisher_info = ''

                # å‘ä¸ŠæŸ¥æ‰¾åŒ…å«è¯„åˆ†çš„å…ƒç´ 
                parent = link.parent
                for _ in range(3):
                    if parent:
                        rating_elem = parent.find('span', class_='rating_nums')
                        if rating_elem:
                            try:
                                rating = float(rating_elem.get_text(strip=True))
                                break
                            except:
                                pass

                        cast_elem = parent.find('span', class_='subject-cast')
                        if cast_elem:
                            cast_text = cast_elem.get_text(strip=True)
                            parts = [p.strip() for p in cast_text.split('/')]
                            if len(parts) >= 1:
                                author_info = parts[0]
                            if len(parts) >= 2:
                                publisher_info = parts[1]
                            break

                        parent = parent.parent

                return {
                    'title': clean_title,
                    'author': author_info or author or '',
                    'publisher': publisher_info,
                    'rating': rating,
                    'url': href,
                    'source': 'link_context'
                }

        except Exception as e:
            print(f"é“¾æ¥ä¸Šä¸‹æ–‡æå–å¤±è´¥: {e}")

        return None

    def _text_contains_title(self, text: str, title: str) -> bool:
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«æ ‡é¢˜"""
        text_clean = text.replace('[ä¹¦ç±]', '').strip().lower()
        title_clean = title.strip().lower()
        return title_clean in text_clean or text_clean in title_clean

    def _search_douban_book(self, title: str, author: str = None) -> Optional[Dict]:
        """é€šè¿‡è±†ç“£è¯»ä¹¦é¡µé¢æœç´¢ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        try:
            query = urllib.parse.quote(title)
            book_search_url = f"https://book.douban.com/subject_search?search_text={query}"

            print(f"å°è¯•è±†ç“£è¯»ä¹¦æœç´¢: {book_search_url}")

            # ä¼˜åŒ–çš„é‡è¯•æœºåˆ¶ï¼šæœ€å¤š1æ¬¡é‡è¯•
            max_retries = 1
            for attempt in range(max_retries + 1):
                try:
                    timeout = 5 if attempt == 0 else 7  # ä½¿ç”¨æ›´çŸ­çš„è¶…æ—¶
                    response = self.session.get(book_search_url, timeout=timeout)
                    break
                except Exception as e:
                    print(f"è±†ç“£è¯»ä¹¦ç¬¬{attempt + 1}æ¬¡å°è¯•å¤±è´¥: {e}")
                    if attempt == max_retries:
                        raise
                    time.sleep(0.5)  # é‡è¯•ç­‰å¾…å‡å°‘åˆ°0.5ç§’

            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')

                books = soup.find_all('li', class_='subject-item') or soup.find_all('div', class_='pic')
                print(f"è±†ç“£è¯»ä¹¦æ‰¾åˆ° {len(books)} ä¸ªç»“æœ")

                for book in books[:3]:
                    book_info = self._extract_book_detail(book, title)
                    if book_info:
                        return book_info

        except Exception as e:
            print(f"è±†ç“£è¯»ä¹¦æœç´¢å¼‚å¸¸: {e}")

        return None

    def _extract_book_detail(self, book_elem, target_title: str) -> Optional[Dict]:
        """ä»è±†ç“£è¯»ä¹¦é¡µé¢æå–ä¹¦ç±è¯¦æƒ…"""
        try:
            title_link = book_elem.find('a', href=True)
            if not title_link:
                return None

            found_title = title_link.get_text(strip=True)
            clean_title = found_title.strip()

            if self._is_title_match(target_title, clean_title):
                book_url = title_link.get('href', '')

                if book_url and not book_url.startswith('http'):
                    book_url = 'https://book.douban.com' + book_url

                return {
                    'title': clean_title,
                    'author': '',
                    'publisher': '',
                    'rating': None,
                    'url': book_url,
                    'source': 'douban_book'
                }

        except Exception as e:
            print(f"æå–è±†ç“£è¯»ä¹¦è¯¦æƒ…å¤±è´¥: {e}")

        return None

    def _is_title_match(self, search_title: str, found_title: str) -> bool:
        """åˆ¤æ–­æ ‡é¢˜æ˜¯å¦åŒ¹é…"""
        search_clean = search_title.strip().lower()
        found_clean = found_title.strip().lower()

        for mark in ['[ä¹¦ç±]', '(è±†ç“£)', 'ï¼ˆè±†ç“£ï¼‰']:
            found_clean = found_clean.replace(mark.lower(), '').strip()

        if search_clean == found_clean:
            return True

        if search_clean in found_clean or found_clean in search_clean:
            min_len = min(len(search_clean), len(found_clean))
            max_len = max(len(search_clean), len(found_clean))
            if min_len / max_len >= 0.7:
                return True

        return False

    def _create_fallback_result(self, title: str, author: str = None, publisher: str = None) -> Dict:
        """åˆ›å»ºå…œåº•ç»“æœï¼Œç¡®ä¿æ€»æœ‰è¿”å›å€¼"""
        return {
            'title': title,
            'author': author or '',
            'publisher': publisher or '',
            'rating': None,
            'url': f"https://www.douban.com/search?cat=1001&q={urllib.parse.quote(title)}",
            'source': 'fallback',
            'note': 'è±†ç“£æœç´¢æš‚æ—¶ä¸å¯ç”¨ï¼Œå·²è®°å½•ä¹¦ç±ä¿¡æ¯'
        }

    def _get_short_comments(self, book_url: str, limit: int = 3) -> list:
        """
        ä»è±†ç“£ä¹¦ç±é¡µé¢è·å–çŸ­è¯„

        Args:
            book_url: è±†ç“£ä¹¦ç±è¯¦æƒ…é¡µURL
            limit: è·å–çš„çŸ­è¯„æ•°é‡ï¼Œé»˜è®¤3æ¡

        Returns:
            çŸ­è¯„åˆ—è¡¨ï¼Œæ¯æ¡çŸ­è¯„åŒ…å«: content(å†…å®¹), author(ä½œè€…), rating(è¯„åˆ†), useful_count(æœ‰ç”¨æ•°)
        """
        try:
            print(f"å¼€å§‹è·å–çŸ­è¯„: {book_url}")

            # ç¡®ä¿URLæ˜¯æœ‰æ•ˆçš„è±†ç“£ä¹¦ç±é¡µé¢
            if not book_url or 'book.douban.com/subject' not in book_url:
                print("æ— æ•ˆçš„è±†ç“£ä¹¦ç±URL")
                return []

            # è¯·æ±‚ä¹¦ç±è¯¦æƒ…é¡µ
            response = self.session.get(book_url, timeout=15)
            if response.status_code != 200:
                print(f"è·å–ä¹¦ç±é¡µé¢å¤±è´¥: {response.status_code}")
                return []

            soup = BeautifulSoup(response.text, 'html.parser')
            comments = []

            # æŸ¥æ‰¾çŸ­è¯„åŒºåŸŸ - è±†ç“£çš„çŸ­è¯„é€šå¸¸åœ¨ id="comments-section" æˆ– class="comment-item"
            comment_items = soup.find_all('div', class_='comment-item') or \
                           soup.find_all('li', class_='comment-item') or \
                           soup.find_all('div', class_='comment')

            print(f"æ‰¾åˆ° {len(comment_items)} æ¡è¯„è®º")

            for item in comment_items[:limit]:
                try:
                    # æå–è¯„è®ºå†…å®¹
                    comment_content = None
                    content_elem = item.find('span', class_='short') or \
                                  item.find('p', class_='comment-content') or \
                                  item.find('div', class_='comment-content')

                    if content_elem:
                        comment_content = content_elem.get_text(strip=True)

                    if not comment_content:
                        continue

                    # æå–ä½œè€…
                    author = ''
                    author_elem = item.find('a', class_='name') or \
                                 item.find('span', class_='comment-info') or \
                                 item.find('a', href=re.compile(r'/people/'))
                    if author_elem:
                        author = author_elem.get_text(strip=True)

                    # æå–è¯„åˆ†
                    rating = None
                    rating_elem = item.find('span', class_=re.compile(r'allstar\d+rating'))
                    if rating_elem:
                        rating_class = rating_elem.get('class', [])
                        for cls in rating_class:
                            if 'allstar' in cls:
                                # allstar50rating -> 5æ˜Ÿ, allstar40rating -> 4æ˜Ÿ
                                match = re.search(r'allstar(\d)0rating', cls)
                                if match:
                                    rating = int(match.group(1))
                                    break

                    # æå–"æœ‰ç”¨"æ•°
                    useful_count = 0
                    useful_elem = item.find('span', class_='vote-count') or \
                                 item.find('span', class_='votes')
                    if useful_elem:
                        try:
                            useful_count = int(useful_elem.get_text(strip=True))
                        except:
                            pass

                    comments.append({
                        'content': comment_content,
                        'author': author,
                        'rating': rating,
                        'useful_count': useful_count
                    })

                except Exception as e:
                    print(f"è§£æå•æ¡è¯„è®ºå¤±è´¥: {e}")
                    continue

            print(f"æˆåŠŸæå– {len(comments)} æ¡çŸ­è¯„")
            return comments

        except Exception as e:
            print(f"è·å–çŸ­è¯„å¤±è´¥: {e}")
            return []